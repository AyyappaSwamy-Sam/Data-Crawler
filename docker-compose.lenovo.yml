version: '3.8'

# Docker Compose for Lenovo Laptop (RTX 4070, i9, 32GB RAM)
# Services: LLM Service with Gemma 3-4B

services:
  # LLM service with Gemma 3-4B model
  llm_service:
    build:
      context: ./gpu_services/llm_service
      dockerfile: Dockerfile.gpu
    container_name: llm_service
    ports:
      - "8001:8001"
    volumes:
      - hf_cache:/root/.cache/huggingface
      - llm_logs:/app/logs
    environment:
      - MODEL_NAME=google/gemma-3-4b-it
      - DEVICE=cuda
      - TORCH_DTYPE=bfloat16
      - GPU_MEMORY_FRACTION=0.95
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: 2gb  # Increase shared memory for large models

volumes:
  hf_cache:
  llm_logs:

networks:
  default:
    name: llm_network
    external: false