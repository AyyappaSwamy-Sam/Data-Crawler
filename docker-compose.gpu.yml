version: '3.8'

# This file is for the Linux GPU Worker Node (192.168.100.43)

services:
  # Milvus Standalone for vector storage
  milvus:
    # --- THIS IS THE ONLY LINE THAT HAS CHANGED ---
    image: milvusdb/milvus:v2.6.0
    container_name: milvus_service
    ports:
      - "19530:19530"
      - "9091:9091"
    volumes:
      - milvus_data:/milvus/data
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000

  # Dedicated service for the main LLM, powered by vLLM
  llm_service:
    build:
      context: ./gpu_services/llm_service
      dockerfile: Dockerfile.gpu
    container_name: llm_service
    ports:
      - "8001:8001"
    volumes:
      - hf_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      python -m vllm.entrypoints.openai.api_server
      --host 0.0.0.0
      --port 8001
      --model google/gemma-3n-E2B-it
      --trust-remote-code
      --gpu-memory-utilization 0.90

  # GPU-powered service for document extraction
  docling_service:
    build:
      context: ./gpu_services/docling_service
      dockerfile: Dockerfile.gpu
    container_name: docling_service
    ports:
      - "8004:8004"
    env_file:
      - ./gpu_services/docling_service/.env
    volumes:
      - ./data:/app/data
      - docling_cache:/root/.cache/docling
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Dedicated GPU-powered service for generating embeddings
  embedding_service:
    build:
      context: ./gpu_services/embedding_service
      dockerfile: Dockerfile.gpu
    container_name: embedding_service
    ports:
      - "8002:8002"
    volumes:
      - hf_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Service that calls our internal llm_service and embedding_service
  knowledge_graph_service:
    build:
      context: ./gpu_services/knowledge_graph_service
      dockerfile: Dockerfile.gpu
    container_name: knowledge_graph_service
    ports:
      - "8003:8003"
    env_file:
      - ./gpu_services/knowledge_graph_service/.env
    depends_on:
      - llm_service
      - embedding_service

volumes:
  milvus_data:
  docling_cache:
  hf_cache: